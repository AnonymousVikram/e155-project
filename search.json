[
  {
    "objectID": "team.html",
    "href": "team.html",
    "title": "Team",
    "section": "",
    "text": "Meet the Team!\n\nTroy Kaufman: Troy is an engineering major with a passion for digital design, and loads of experience with embedded system projects. Find him on LinkedIn!\nVikram Krishna: Vikram is a junior double-majoring in Engineering and CS. This year, he is working on the 10xEngineers clinic project to develop RISC-V verification tools. Find him on LinkedIn!\nJoseph Abdelmalek: Joseph is a junior engineering major looking at a future career in medical devices. Find him on LinkedIn!"
  },
  {
    "objectID": "mcu-design.html",
    "href": "mcu-design.html",
    "title": "MCU Design",
    "section": "",
    "text": "Current Design\nIn the project, the MCU was designed to parse and transfer pixel data. A python script grayscaled and downsized a JPEG image so that all the pixel data could fit in the MCU’s SRAM successfully upon runtime at a size of 320x240 pixels. In order for edge detection to be properly applied in the FPGA, the MCU needed to transmist a 3x3 kernal of nibbles. This was perfomed by sending a series of 12-bit packets over the SPI peripheral where each represented one of the image’s columns and within that three consecutive rows of data.\n\n\n\nThe MCU Transferring a 12-bit Data packet to the FPGA to create the 3x3 Kernal\n\n\nAfter each kernal is transmitted, the new kernal of data would shift over one column within the image, thus, sending a new 3x3 kernal. Once the the kernal has reached the end of the 240 columns, the matrix would be reset back to the 0th column and shifted down by 1 row.\n\n\nOriginal Design\nDue to complications in communicating with the ArduCAM Mini 2MP Plus camera module, the project had to downsize heavily on the MCU part. Originally, the I2C peripheral would configure two register banks within the OV2640 camera to alter the resolution and output data format to VGA and RGB565 respectively. After the camera was configured, a SPI peripheral would retrieve pixel data from the module.\nTo prevent data from being overwritten, a DMA controller faciliated communication between the camera module and the MCU without the need for the processor to intervene. This allowed the processor to focus on other tasks effectively creating a semi-concurrent system. Two DMA channels were used where one was a memory to peripheral transfer system to send clock and fifo-read packets to the camera module while the second channel was a peripheral to memory double buffer reception system. An interrupt was triggered whenever one of the buffers experienced a transaction completed event.\nThe buffer that triggered the event would then transfer data to the processor to be converted from RGB565 to grayscale data - a critical step to the overall edge detection method. This is where the current system design would be used in choosing specific nibbles to create the 3x3 kernal would be implemented as described above. A third DMA channel, a memory to peripheral transaction, would retrieve the processed data and send it to the FPGA to develop the kernal with a total of three 12-bit data packets.\n\n\nCamera Module Reflection\nAs mentioned before, the camera module would not deliver reliable output data. At times it would deliver valid data, but more often than not, gibberish would be received. This is more likely due to incorrectly configurating the camera module. We manually tried to configure the camera with known register settings with some success. We also reconfigured a public library for a STM32 ArduCAM Mini 2MP Plus camera module congigurations to no avail. On the vendor’s website, we noticed that there was conflicting documentation concerning the SPI peripheral’s clock phase. If there was a major mistake like this, why wouldn’t there be more mistakes within the documentation? This led the team to drop the camera module from the project two days before the official checkoff.\n\n\n\nThe Camera Module’s Pixel Data in RGB565 Format Showing a Forest Green Color (0x2A and 0xA2) as Desired\n\n\nIf we were able to redo this project, we would have invested in a better camera with more reliable documentation. It was unfortunate that the team was not able to correctly implement the camera module."
  },
  {
    "objectID": "fpga-design.html",
    "href": "fpga-design.html",
    "title": "FPGA Design",
    "section": "",
    "text": "FPGA (Field Programmable Gate Array) is a type of integrated circuit that can be programmed after manufacturing. It is a reconfigurable device that can be used to implement digital logic circuits. FPGAs are used in a wide range of applications, including digital signal processing, networking, and embedded systems.\nOne of the big advantages of FPGAs is that they can be reprogrammed to implement different functions. This makes them very flexible and allows designers to quickly prototype and test new designs. FPGAs are also very fast and can be used to implement high-performance digital systems.\n\n\nThe Sobel edge detection algorithm is a popular image processing technique that is used to detect edges in images. It works by convolving the image with a pair of 3x3 convolution kernels to calculate the gradient of the image intensity at each pixel. The magnitude of the gradient is then used to determine if the pixel is part of an edge.\nThe kernel is weighed and can be seen below:\n-1  0  1\n-2  0  2\n-1  0  1\n\n\n\nVGA (Video Graphics Array) is a standard for displaying video on a computer monitor. It uses an analog signal to transmit the video data to the monitor. VGA displays are commonly used in embedded systems and other applications where a high-quality display is required."
  },
  {
    "objectID": "fpga-design.html#sobel-edge-detection",
    "href": "fpga-design.html#sobel-edge-detection",
    "title": "FPGA Design",
    "section": "",
    "text": "The Sobel edge detection algorithm is a popular image processing technique that is used to detect edges in images. It works by convolving the image with a pair of 3x3 convolution kernels to calculate the gradient of the image intensity at each pixel. The magnitude of the gradient is then used to determine if the pixel is part of an edge.\nThe kernel is weighed and can be seen below:\n-1  0  1\n-2  0  2\n-1  0  1"
  },
  {
    "objectID": "fpga-design.html#vga-display",
    "href": "fpga-design.html#vga-display",
    "title": "FPGA Design",
    "section": "",
    "text": "VGA (Video Graphics Array) is a standard for displaying video on a computer monitor. It uses an analog signal to transmit the video data to the monitor. VGA displays are commonly used in embedded systems and other applications where a high-quality display is required."
  },
  {
    "objectID": "fpga-design.html#overall-architecture",
    "href": "fpga-design.html#overall-architecture",
    "title": "FPGA Design",
    "section": "Overall Architecture",
    "text": "Overall Architecture\nTo gain an overarching idea of the FPGA design, Figure 1 below shows the main components of the FPGA design:\n\n\n\n\n\n\nFigure 1: Block Diagram of FPGA Design"
  },
  {
    "objectID": "fpga-design.html#sobel-edge-detection-algorithm",
    "href": "fpga-design.html#sobel-edge-detection-algorithm",
    "title": "FPGA Design",
    "section": "Sobel Edge Detection Algorithm",
    "text": "Sobel Edge Detection Algorithm\nThe edge detection algorithm was implemented using a 5-stage pipeline. The pipeline was designed to process one pixel per clock cycle, allowing for high throughput and efficient computation. The pipeline stages were as follows:\n\nSum of Rows and Columns: The input pixel values were multiplied by the Sobel kernel values and summed to calculate the gradient in the x and y directions.\nGradient Magnitude: The gradient values in the x and y directions were calculated from the sum of rows and columns.\nSquaring and Summing: The gradient values were squared and summed to calculate the magnitude of the gradient.\nSquare Root: The square root of the gradient magnitude was calculated to determine if the pixel was part of an edge.\nBit Compression: The gradient magnitude was compressed to 2 bits to reduce the amount of data that needed to be transmitted to the VGA monitor and stored in memory.\n\nThe first three steps were relatively straightforward, with basic arithmetic using adders and multipliers. The square root calculation was more complex and required a lookup table to approximate the square root function. To that end, a Python script was created to generate the square roots for all values from 0 - 7200. The maximum value was calculated by inputting theoretical maximum gradient values into the Sobel kernel. This lookup table was then stored in the BRAM on the FPGA for quick near-instantaneous access.\nFurther, the Bit Compression step was implemented to reduce the amount of data that needed to be transmitted to the VGA monitor and stored in memory. The output of the square root BRAM was compressed to 2 bits using a simple thresholding function:\n\\(\\text{Output} = \\begin{cases} 00 & \\text{Input} \\leq 21 \\\\ 01 & 21 &lt; \\text{Input} \\leq 42 \\\\ 10 & 42 &lt; \\text{Input} \\leq 63 \\\\ 11 & \\text{Input}  &gt; 63 \\end{cases}\\)\nThese threshold values were determined as the maximum output of the square root function was 84. So, despite being stored in a 6-bit register with a theoretical maximum of 127, these values maximize the dynamic range of the output.\nBy putting registers in between each stage of the pipeline, the FPGA was able to process one pixel per clock cycle, allowing for high throughput and efficient computation. Although with the time delay of receiving new image data from the MCU over SPI this would be slowed down, registers are also added to store the current x and y pixel values to ensure that the correct pixel is processed and stored in the SPRAM."
  },
  {
    "objectID": "fpga-design.html#vga-output-module",
    "href": "fpga-design.html#vga-output-module",
    "title": "FPGA Design",
    "section": "VGA Output Module",
    "text": "VGA Output Module\nThe VGA output module was responsible for displaying the processed image data on a VGA monitor. The module was designed to output a 640x480 image with a resolution of 800x600 pixels. The module used a single-port RAM to store the processed image data and a VGA controller to generate the VGA signal.\nThe VGA controller generated the horizontal and vertical sync signals and the pixel clock signal required by the VGA monitor. The controller also generated the pixel address for the RAM based on the current horizontal and vertical sync signals. The processed image data was read from the RAM and outputted to the VGA monitor.\nThe VGA output module was implemented using a state machine that controlled the timing of the VGA signals and the reading of the processed image data from the RAM."
  },
  {
    "objectID": "fpga-design.html#spram-logic",
    "href": "fpga-design.html#spram-logic",
    "title": "FPGA Design",
    "section": "SPRAM Logic",
    "text": "SPRAM Logic\nAlthough reading/writing from a SPRAM seems straightforward, the logic to enable this functionality actually ended up being non-trivial. This is primarily due to the strict timing constraints of the VGA output module. The VGA output module requires a pixel to be output every 40ns, which is the time it takes for the VGA monitor to display a pixel. This meant that the clock going into the SPRAM had to be twice as fast, at 50MHz. Further, the SPRAM had a write mask input that allows the user to write to specific 4-bit fields in the SPRAM. However, since we decided to store 2 bits per pixel (4 bits would’ve resulted in not enough SPRAM), we had to write to the SPRAM in 2 cycles. This resulted in a 4-state FSM that looked like this:\n\n\n\n\n\ngraph TD\n    A((VGA Read)) --&gt; B((Load Line))\n    B --&gt; C((VGA Read))\n    C --&gt; D((Write to RAM))\n    D --&gt; A\n\n\n\n\n\n\nThere was also a buffer system implemented to account for the fact that there are no guarantees that the SPI clock would divide the FPGA’s clock evenly. This buffer system would store the pixel data in a buffer any time an input is detected, and it would be processed and stored in the RAM in the next possible FSM cycle."
  },
  {
    "objectID": "fpga-design.html#spi-communication",
    "href": "fpga-design.html#spi-communication",
    "title": "FPGA Design",
    "section": "SPI Communication",
    "text": "SPI Communication\nThe SPI IP provided by Lattice was not ideal for our purposes, and as a result, we designed our own SPI module as well. Taking in a clock, and a single data line, the SPI module would read in data from the MCU and store it in registers. These registers would then be processed by the Sobel edge detection algorithm, and stored in the SPRAM."
  },
  {
    "objectID": "documentation.html",
    "href": "documentation.html",
    "title": "Documentation",
    "section": "",
    "text": "Our new piece of hardware that we attempted to incorporate into our design was the ArduCAM Mini 2MP camera module that incoprated an OV2640 camera. The camera itself has two internal bank registers that allowed the user to configure the camera settings such as resolution and output format. The module produce by ArduCAM incorporated another bank of registers that allowed users to quickly configure and access data.\nIn terms of hardware for our project, we used an FPGA and MCU. Our bill of materials is produced below:\n\n\n\nName\nDescription\nManufacturer\nPart Number/ID\nQuantity\nUnit Price\nLinks\n\n\n\n\nVGA Breakout Board\nFemale Plug to Terminal Block Breakout\nAdafruit\n3124\n1\n$4.50\nAdafruit.com\n\n\nVGA Connector\nMale connectors on either end\nStockroom\nN/A\n1\nStockroom\n\n\n\nArudCAM Mini 2MP Camera Module\n2MP Camera\nArduCAM\nB0067\n1\n$26\nArduCAM.com\n\n\n\nOur electrical schematic is shown below:\n\n\n\n\nSchematic for Edge Detection System\n\n\n\nOur published source code can be found in our Github repository here.\n\n\n\n\nFPGA Block Diagram"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "E155 Final Project Portfolio Website",
    "section": "",
    "text": "For our final MicroPs project, we combined an FPGA and a microcontroller (MCU) to create an edge detection filter, capable of processing images and outputting the results to a VGA monitor. We chose to process our image data by separating the image capture and image processing to the MCU and FPGA respectively. The MCU handles the image preparation processes like greyscaling to get it ready for edge detection. It then sends the input image data to the FPGA through SPI and allows users to see the edge-detection. The FPGA handles the computationally intensive task of edge detection following a Sobel algorithm. The parallel processing capabilities of the FPGA ensure efficient computation. The processed image data, now highlighting edges, is outputted on the monitor via VGA. Our block diagram for our system is reproduced below:\n\n\n\n\nBlock Diagram of Edge Detection System\n\n\n\nCheck out this video to see how it works!\n\nTable of Contents\nWant more information? Check out the other resources below: - Documentation\nA comprehensive overview of the project, including its objectives, scope, and key technical challenges.\n\nFPGA Design\nDetailed insights into the FPGA implementation, including the edge detection algorithm, logic design, and VGA output module.\nMCU Design\nAn explanation of the MCU’s role in system integration, user interface management, and data handling.\nResults\nA presentation of the project’s outcomes, including test results, performance benchmarks, and processed images.\nTeam\nMeet the team behind the project, with details on individual contributions and collaboration efforts.\n\nA special thanks to Prof. Brake and the MicroPs grutors for helping us with this amazing project!"
  },
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "Results",
    "section": "",
    "text": "Project Demonstration\n\n\n\n1 VGA\nThe VGA works as intended where there is 2-bit resolution for each color. Theoretically, that means we have access up to 64 colors. The hSync and vSync signals contributing to the VGA’s timing are extremely accurate. The VGA was one of the earliest specs completed in the project, and it has helped us debug other specs.\n\n\n2 Camera Interface with I2C and SPI\nBefore the camera was scrapped from the project, I2C initialization and write functions worked as intended in setting up the output format. No unknown decoded bits were found while I2C transfered data. The SPI peripheral worked as intended in receiving pixel data from the camera module. However, the ouput data from the camera module was very inconsistent which can be attributed to the configuration setup within the two internal register banks. We manually configured registers by looking at the datasheet, and used a library available to the public. However, the camera’s reliability did not change. We do not use I2C now, but we still use SPI and it works properly.\n\n\n3 Sobel Edge Detection Algorithm\nThe Sobel edge detection implementation proved highly successful, demonstrating both accuracy and exceptional performance. By leveraging the FPGA’s parallel processing capabilities and implementing a 5-stage pipeline architecture, we achieved processing speeds of one pixel per clock cycle. The edge detection quality was enhanced by using a lookup table stored in BRAM for fast square root calculations, and the 2-bit compression scheme effectively preserved edge detail while optimizing memory usage.\nThe algorithm successfully identifies edges across different image intensities, using carefully calibrated thresholds (21, 42, and 63) to maximize the dynamic range of the output. The SPRAM storage system handled the processed image data without any timing violations, ensuring smooth display output to the VGA monitor.\nThe hardware acceleration provided by the FPGA made the edge detection significantly faster than what would be possible with software-only implementation on the MCU, demonstrating the advantages of our hardware/software partitioning approach.\n\n\n4 DMA and Double Buffer\nBefore the camera was scrapped from the project the DMA and double buffer system worked worked well. In total, three DMA channels were utilized (2x memory to peripheral and 1x peripheral to memory). The first memory to peripheral DMA transaction sends clock and read fifo command packets to the camera module. The one and only peripheral to memory DMA transaction used a doubler buffer system to avoid overwriting data. A handful of pointers were used in this operation, and data transfers worked seamlessly within memory. The final DMA channel took the preprocessed pixel data and sent it over SPI successfully. However, the camera module was scrapped from the project, so we are not using the DMA controller nor double buffer system currently.\n\n\n5 SPRAM, BRAM, and SPI on FPGA\nThe SPRAM, BRAM, and SPI on the FPGA work as intended. The square root look up table fits nicely within the BRAM. The SPRAM holds the entire edge detected image without conflict, and it can be read and written to without causing any setup time violations when synthesized."
  }
]